# Data Pipelines with Airflow
## Project Overview
A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.
For that purpose, we are creating a high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

## Prerequisites
Create an IAM User in AWS
Configure Redshift Serverless in AWS.
In Airflow UI, configure `aws credential` and `redshift credentials` 

## Datasets
We are using two datasets, log_data & song_date, for this purpose

### Song dataset: s3://udacity-dend/song_data
  The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
  eg: s3.ObjectSummary(bucket_name='udacity-dend', key='song-data/A/A/C/TRAACMJ128F930C704.json')
      s3.ObjectSummary(bucket_name='udacity-dend', key='song-data/A/B/B/TRABBDN12903D0571D.json')
      s3.ObjectSummary(bucket_name='udacity-dend', key='song-data/K/J/Y/TRKJYBX128F426EF4C.json')
And below is an example of what a single song file, TRAACMJ128F930C704.json, looks like

```{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0} ```

### Log dataset: s3://udacity-dend/log_data
  The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset are partitioned by year and month. 
  eg: s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-01-events.json')
      s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-02-events.json')
      s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-03-events.json')
      
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like
      ![image](https://github.com/anwesha-git/redshift/assets/122990634/300ba991-0a72-4690-8354-f865b20cd47e)

This third file s3://udacity-dend/log_json_path.json contains the meta information that is required by AWS to correctly load s3://udacity-dend/log_data. And below is what data is in log_json_path.json.

![image](https://github.com/anwesha-git/redshift/assets/122990634/f5c1fadf-0621-4c41-bd3d-42d11c1f260f)

## Project Template
The project template includes below structure:

airflow<br />
├── dags<br />
│   └── sparkify_data_pipeline.py<br />
└── plugins<br />
    ├── operators<br />
    │   ├── create_table.py<br />
    │   ├── stage_redshift.py<br />
    │   ├── load_fact.py<br />
    │   ├── load_dimension.py<br />
    │   └── data_quality.py   <br />
    └── helpers<br />
        └── sql_queries.py <br />
        └── sql_create_queries.py <br />
        └── create_tables.py <br />
        
### Airflow Pipeline
The DAG should appear as "sparkify_data_pipeline".
On the pipeline, the following sequence will be followed:
1. Both the staging data tables will be copied from S3 to Redshift<br />
   `staging_song` and `staging_event`
2. The Fact table will be loaded from both staging tables to redshift<br />
    There is a truncate parameter, if set true, it will do truncate load, else it will append<br />
   `songplays`
3. The dimension tables will get loaded from both staging tables to redshift<br />
    There is a truncate parameter, if set true, it will do truncate load, else it will append<br />
   `songs`, `artists`, `time`, `users`
4. Quality check will be run to ensure the data is loaded successfully<br />

![image](https://github.com/anwesha-git/airflow/assets/122990634/2d2a8868-6cf5-4e30-b9bb-b773b9fc98dc)
   

